{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6c054715-2fd3-4094-bdf8-07f0c25d31ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import inspect\n",
    "from threading import Thread\n",
    "from pyspark.sql import SparkSession\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.functions import col, regexp, current_user, asc, lower, nvl, lit\n",
    "\n",
    "spark = None\n",
    "warehouse_id = None\n",
    "w = WorkspaceClient()\n",
    "cluster_id = '0627-133437-mtg2jcrp'\n",
    "\n",
    "\n",
    "class ThreadWithReturnValue(Thread):\n",
    "    def __init__(self, group=None, target=None, name=None, args=[], kwargs={}, Verbose=None):\n",
    "        Thread.__init__(self, group, target, name, args, kwargs)\n",
    "        self._return = None\n",
    "\n",
    "    def run(self):\n",
    "        if self._target is not None:\n",
    "            try:\n",
    "                self._return = self._target(*self._args, **self._kwargs)\n",
    "            except Exception as ex:\n",
    "                self._return = Exception(f'Exception in {self._target.__name__}: {ex}')\n",
    "\n",
    "    def join(self, *args):\n",
    "        Thread.join(self, *args)\n",
    "        return self._return\n",
    "\n",
    "\n",
    "class ThreadList(list):\n",
    "    def append(self, thread) -> None:\n",
    "        if not isinstance(thread, ThreadWithReturnValue):\n",
    "            raise TypeError(f'thread must be of an instance of ThreadWithReturnValue not {type(thread)}')\n",
    "        return super().append(thread)\n",
    "    \n",
    "    def start_all_threads(self) -> None:\n",
    "        for thread in self:\n",
    "            thread.start()\n",
    "\n",
    "    def join_all_threads(self) -> dict:\n",
    "        final_res = {}\n",
    "        exception_lst = []\n",
    "        for thread in self:\n",
    "            res = thread.join()\n",
    "            if res and isinstance(res, dict):\n",
    "                final_res.update(res)\n",
    "            elif res and isinstance(res, Exception):\n",
    "                exception_lst.append(res)\n",
    "        \n",
    "        if exception_lst:\n",
    "            for i, exception in enumerate(exception_lst, start=1):\n",
    "                print(f'---- EXCEPTION #{i} START ----\\n\\n{str(exception).strip()}\\n\\n---- EXCEPTION #{i} END ----')\n",
    "                if i < len(exception_lst):\n",
    "                    print('\\n\\n')\n",
    "            print()\n",
    "            raise Exception('GOT THE ABOVE EXCEPTIONS PLEASE REVIEW')\n",
    "        \n",
    "        return final_res\n",
    "\n",
    "    def start_and_join_all_threads(self):\n",
    "        self.start_all_threads()\n",
    "        return self.join_all_threads()\n",
    "\n",
    "\n",
    "class Table():\n",
    "    def __init__(self, full_tbl_name, full_copy_name: str=None, exclude_cols: list=None, col_mapping: dict=None, column_exprs: dict=None, data_type_exprs: dict=None) -> None:\n",
    "        self.full_tbl_name = full_tbl_name\n",
    "        self.catalog_name, self.schema_name, self.tbl_name = full_tbl_name.split('.')\n",
    "        \n",
    "        if full_copy_name is not None:\n",
    "            self.full_copy_name = full_copy_name.replace('%CATALOG_NAME%', self.catalog_name).replace('%SCHEMA_NAME%', self.schema_name).replace('%TBL_NAME%', self.tbl_name)\n",
    "            self.copy_catalog_name, self.copy_schema_name, self.copy_tbl_name = self.full_copy_name.split('.')\n",
    "            assert self.full_copy_name != self.full_tbl_name, f'TABLE NAME: {self.full_tbl_name} CANNOT EQUAL COPY NAME: {self.full_copy_name}'\n",
    "        else:\n",
    "            self.full_copy_name, self.copy_catalog_name, self.copy_schema_name, self.copy_tbl_name = None, None, None, None\n",
    "\n",
    "        self.exclude_cols = []\n",
    "        if exclude_cols is not None:\n",
    "            self.exclude_cols = [col.lower() for col in exclude_cols]\n",
    "\n",
    "        self.col_mapping = {}\n",
    "        if col_mapping is not None:\n",
    "            self.col_mapping = {from_col.lower(): to_col.lower() for from_col, to_col in col_mapping.items()}\n",
    "\n",
    "        self.column_exprs = {}\n",
    "        if column_exprs is not None:\n",
    "            self.column_exprs = {dt: expr for dt, expr in column_exprs.items()}\n",
    "\n",
    "        self.data_type_exprs = {}\n",
    "        if data_type_exprs is not None:\n",
    "            self.data_type_exprs = {dt: expr for dt, expr in data_type_exprs.items()}\n",
    "\n",
    "        self.col_info = self.get_tbl_col_info()\n",
    "\n",
    "\n",
    "    def get_tbl_col_info(self) -> dict:\n",
    "        col_info = []\n",
    "        for idx, row in enumerate(execute_sql_query(f'DESC {self.full_tbl_name}', as_df=True).collect()):\n",
    "            if row['col_name'] == '# Partition Information':\n",
    "                break\n",
    "            if row['col_name'].lower() in self.exclude_cols:\n",
    "                continue\n",
    "\n",
    "            tmp = {'name': row['col_name'].lower(), 'data_type': row['data_type'].lower(), 'idx': idx}\n",
    "            if tmp['name'] in self.col_mapping:\n",
    "                tmp['alias'] = self.col_mapping[tmp['name']]\n",
    "\n",
    "            # If the column has a expr\n",
    "            if self.column_exprs.get(tmp['name'], '') != '':\n",
    "                tmp['col_expr'] = self.column_exprs[tmp['name']]\n",
    "            \n",
    "            # If the columns datatype has an expr\n",
    "            if self.data_type_exprs.get(tmp['data_type'], '') != '':\n",
    "                tmp['dt_expr'] = self.data_type_exprs[tmp['data_type']]\n",
    "\n",
    "            col_info.append(tmp)\n",
    "        return col_info\n",
    "    \n",
    "\n",
    "    def get_col_lst(self, aliased: bool=False) -> list:\n",
    "        return [col.get('alias', col['name']) if aliased else col['name'] for col in self.col_info]\n",
    "\n",
    "\n",
    "    # def get_col_csv(self, aliased: bool=False, use_dt_expr: bool=False):\n",
    "    #     ret = []\n",
    "    #     for col in self.col_info:\n",
    "    #         col_name = col['name']\n",
    "    #         col_dt = col['data_type']\n",
    "\n",
    "    #         # If the columns datatype has an expr\n",
    "    #         dt_expr = self.data_type_exprs[col_dt.lower()].format(__COLUMN_NAME__=col_name) if use_dt_expr and col_dt in self.data_type_exprs else ''\n",
    "\n",
    "    #         str_ret = dt_expr if dt_expr else col_name\n",
    "\n",
    "    #         # If the column has an alias add that if the alised flag is true\n",
    "    #         # If there is a dt_expr and the alias is false we only want the column name\n",
    "    #         str_ret += f\" AS {col.get('alias', col_name) if aliased else col_name}\" if aliased or dt_expr else ''\n",
    "            \n",
    "    #         ret.append(str_ret)\n",
    "            \n",
    "    #     return ','.join(ret)\n",
    "\n",
    "    def get_col_expr_str(self, col, aliased: bool=False, use_col_expr: bool=False, use_dt_expr: bool=False):\n",
    "        col_name = col['name']\n",
    "\n",
    "        ret = f'{col_name}'\n",
    "        if use_col_expr and 'col_expr' in col:\n",
    "            ret = col['col_expr'].format(__COLUMN_NAME__=ret)\n",
    "\n",
    "        if use_dt_expr and 'dt_expr' in col:\n",
    "            ret = col['dt_expr'].format(__COLUMN_NAME__=ret)\n",
    "        \n",
    "        if aliased and 'alias' in col:\n",
    "            ret += f\" AS {col['alias']}\"\n",
    "        elif (use_col_expr and 'col_expr' in col) or (use_dt_expr and 'dt_expr' in col):\n",
    "            ret += f\" AS {col_name}\"\n",
    "\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def get_col_csv(self, aliased: bool=False, use_col_expr: bool=False, use_dt_expr: bool=False):\n",
    "        return ','.join([self.get_col_expr_str(col, aliased=aliased, use_col_expr=use_col_expr, use_dt_expr=use_dt_expr) for col in self.col_info])\n",
    "    \n",
    "\n",
    "    def get_copy_ddl(self, copy_type: str, filter_clause: str=None, aliased: bool=False) -> str:\n",
    "        copy_type = copy_type.upper().strip()\n",
    "        assert copy_type in ('TABLE', 'VIEW'), f'INVALID COPY TYPE: {copy_type}, ONLY TABLE AND VIEW ARE SUPPORTED'\n",
    "        ddl = f'CREATE OR REPLACE {copy_type} {self.full_copy_name} AS SELECT {self.get_col_csv(aliased=aliased, use_col_expr=True, use_dt_expr=True)} FROM {self.full_tbl_name}'\n",
    "        if filter_clause:\n",
    "            ddl += filter_clause\n",
    "        return ddl\n",
    "    \n",
    "\n",
    "    def set_exclude_cols(self, exclude_cols: list) -> None:\n",
    "        new_exclude_cols = [col.lower() for col in exclude_cols]\n",
    "        if len(list(set(self.exclude_cols) - set(new_exclude_cols))):\n",
    "            self.exclude_cols = new_exclude_cols\n",
    "            self.col_info = self.get_tbl_col_info()\n",
    "\n",
    "\n",
    "def validate_function_args(func, args:dict) -> None:\n",
    "    func_params = inspect.signature(func).parameters\n",
    "    for arg_name in func_params:\n",
    "        arg = func_params[arg_name]\n",
    "        arg_hint_type, arg_default = arg.annotation, arg.default\n",
    "        \n",
    "        # This must mean it is a required arg since it does not have a default\n",
    "        # if arg.default == arg.empty:\n",
    "        #     print(arg)\n",
    "\n",
    "        # Make sure the type matches, unless it equals the default value\n",
    "        if not isinstance(args[arg_name], arg_hint_type) and arg_default != args[arg_name]:\n",
    "            raise TypeError(f'Invalid type for arg {arg_name}, expected {arg_hint_type}, got {type(args[arg_name])}')\n",
    "\n",
    "\n",
    "def execute_sql_query(query: str, as_df: bool=False, warehouse_id=None):\n",
    "    if warehouse_id is None:\n",
    "        if 'warehouse_id' not in globals() or globals()['warehouse_id'] is None:\n",
    "            raise Exception('WHEN IMPORTING execute_sql_query warehouse_id MUST BE PASSED AS A PARAM')\n",
    "        else:\n",
    "            warehouse_id = globals()['warehouse_id']\n",
    "\n",
    "    res = w.statement_execution.execute_statement(query, warehouse_id, wait_timeout='0s')\n",
    "    \n",
    "    wait_states = ['PENDING', 'RUNNING']\n",
    "\n",
    "    try:\n",
    "        time.sleep(1)\n",
    "        res = w.statement_execution.get_statement(res.statement_id)\n",
    "\n",
    "        # Wait for query to complete and cancel it if a keyboard interrupt is detected\n",
    "        while res.status.state.value in wait_states:\n",
    "            res = w.statement_execution.get_statement(res.statement_id)\n",
    "            time.sleep(5)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        w.statement_execution.cancel_execution(res.statement_id)\n",
    "        raise\n",
    "    \n",
    "    if res.status.state.value == 'FAILED':\n",
    "        raise Exception(res.status.error.message)\n",
    "    elif res.status.state.value == 'CANCELED':\n",
    "        raise Exception(f'QUERY HAS BEEN CANCELED, ID: {res.statement_id}')\n",
    "    elif res.status.state.value not in wait_states and res.status.state.value != 'SUCCEEDED':\n",
    "        raise Exception(f'UNKNOWN STATE {res.status.state.value} FOR QUERY ID: {res.statement_id}')\n",
    "\n",
    "    if as_df:\n",
    "        res_dict = res.as_dict()\n",
    "        col_lst = sorted(res_dict['manifest']['schema']['columns'], key=lambda x: x['position'])\n",
    "\n",
    "        # df = pd.DataFrame(\n",
    "        #     res_dict.get('result', {}).get('data_array', [])\n",
    "        #     , columns=[col['name'] for col in col_lst]\n",
    "        # )\n",
    "\n",
    "        # dt_map = {\n",
    "        #     'INT': int\n",
    "        #     , 'BOOLEAN': bool\n",
    "        #     , 'STRING': str\n",
    "        #     , 'DATE': 'datetime64'\n",
    "        #     , 'TIMESTAMP': 'datetime64'\n",
    "        # }\n",
    "        # df = df.astype({col['name']: dt_map[col['type_name']] for col in col_lst}).convert_dtypes()\n",
    "\n",
    "        # Using spark\n",
    "        df = spark.createDataFrame(res_dict.get('result', {}).get('data_array', []), schema=','.join([f\"`{col['name']}` STRING\" for col in col_lst]))\n",
    "        for col in col_lst:\n",
    "            df = df.withColumn(col['name'], df[f\"`{col['name']}`\"].cast(col['type_name']))\n",
    "\n",
    "        return df\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def get_default_arg_map() -> dict:\n",
    "    return {\n",
    "        'db_tbl': {'required': True, 'type': str}\n",
    "        , 'sf_tbl': {'required': True, 'type': str}\n",
    "        , 'filter_clause': {'required': False, 'type': str}\n",
    "        , 'warehouse_id': {'required': False, 'type': str, 'default': '0e9c378a506f69a9'}\n",
    "        # Will set the default for this later, dont set it here\n",
    "        , 'missmatch_tbl_name': {'required': False, 'type': str}\n",
    "        , 'cfg_tbl_name': {'required': False, 'type': str, 'default': 'users.pamons.validation_config'}\n",
    "        , 'sf_copy_obj_type': {'required': False, 'type': str, 'default': 'TABLE'}\n",
    "        , 'db_copy_obj_type': {'required': False, 'type': str, 'default': 'VIEW'}\n",
    "        , 'json_ordering_udf': {'required': False, 'type': str, 'default': 'users.pamons.sort_json_str_keys'}\n",
    "        , 'exclude_cols': {'required': False, 'type': list}\n",
    "        , 'db_exclude_cols': {'required': False, 'type': list}\n",
    "        , 'sf_exclude_cols': {'required': False, 'type': list}\n",
    "        , 'primary_keys': {'required': False, 'type': list}\n",
    "        , 'skip_checks': {'required': False, 'type': list}\n",
    "        # List of dicts\n",
    "        , 'column_mapping': {\n",
    "            'required': False, 'type': list\n",
    "            , 'arg_map': {\n",
    "                'sf': {'required': True, 'type': str} \n",
    "                , 'db': {'required': True, 'type': str} \n",
    "            }\n",
    "        }\n",
    "        , 'column_exprs': {\n",
    "            'required': False, 'type': dict, 'lower_case_keys': True\n",
    "            # Since the keys can differ because they are column names we cannot make an arg map\n",
    "            , 'skip_arg_map_check': True\n",
    "        }\n",
    "        , 'data_type_exprs': {\n",
    "            'required': False, 'type': dict\n",
    "                , 'arg_map': {\n",
    "                    'timestamp': {'required': False, 'type': str} \n",
    "                    , 'date': {'required': False, 'type': str}\n",
    "                }\n",
    "                , 'lower_case_keys': True\n",
    "                , 'default': {'timestamp': 'date_trunc(\"SECOND\", {__COLUMN_NAME__})'}\n",
    "        }\n",
    "        , 'fail_at_first_check': {'required': False, 'type': bool}\n",
    "        , 'include_queries': {'required': False, 'type': bool}\n",
    "        , 'use_threads': {'required': False, 'type': bool}\n",
    "        , 'use_cfg_tbl': {'required': False, 'type': bool}\n",
    "        # 0 meaning no table will be created\n",
    "        , 'num_of_sample_rows': {'required': False, 'type': int, 'default': 0}\n",
    "\n",
    "        # Debug only flags\n",
    "        , 'create_sf_copy': {'required': False, 'type': bool, 'default': True}\n",
    "        , 'drop_sf_copy': {'required': False, 'type': bool, 'default': True}\n",
    "        , 'create_db_copy': {'required': False, 'type': bool, 'default': True}\n",
    "        , 'drop_db_copy': {'required': False, 'type': bool, 'default': True}\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_cfg(cfg: dict, arg_map=None, exclude_checks: list=[]) -> dict:\n",
    "\n",
    "    if arg_map is None:\n",
    "        arg_map = get_default_arg_map()\n",
    "\n",
    "    type_defaults = {\n",
    "        list: []\n",
    "        , bool: False\n",
    "        , str: ''\n",
    "        , dict: {}\n",
    "    }\n",
    "\n",
    "    # Check to make sure we are not missing any required args\n",
    "    missing_required_args = list(set([arg for arg in arg_map if arg_map[arg]['required'] and arg not in exclude_checks]) - set(cfg))\n",
    "    assert not missing_required_args, f'Missing required arguments {missing_required_args}'\n",
    "\n",
    "    # This map will store variable names that used the default value, useful for dicts\n",
    "    used_default_map = {}\n",
    "    for arg in arg_map:\n",
    "        # These checks we should excluded\n",
    "        if arg in exclude_checks:\n",
    "            continue\n",
    "\n",
    "        # Add default values to optional missing args\n",
    "        if arg not in cfg:\n",
    "            if 'default' in arg_map[arg]:\n",
    "                cfg[arg] = arg_map[arg]['default']\n",
    "            else:\n",
    "                cfg[arg] = type_defaults[arg_map[arg]['type']]\n",
    "            used_default_map[arg] = True\n",
    "        \n",
    "        if not isinstance(cfg[arg], arg_map[arg]['type']):\n",
    "            raise TypeError(f\"ARG {arg} HAS TYPE {type(cfg[arg])} BUT IS EXPECTED TO BE {arg_map[arg]['type']}\")\n",
    "\n",
    "        if isinstance(cfg[arg], dict) or (isinstance(cfg[arg], list) and 'arg_map' in arg_map[arg]):\n",
    "            \n",
    "            # Reuse code to validate dicts or lists of dicts, as_dict will convert it back to a dict after\n",
    "            as_dict = False\n",
    "            if isinstance(cfg[arg], dict):\n",
    "                as_dict = True\n",
    "                cfg[arg] = [cfg[arg]]\n",
    "\n",
    "            for i, item in enumerate(cfg[arg]):\n",
    "                # If the default was used and the null_on_default flg is in the arg_map for this var \n",
    "                # we can set args value as null and skip the checks validating the keys/values in the arg\n",
    "                # This usually means the arg was not passed\n",
    "                if used_default_map.get(arg, False) and arg_map[arg].get('null_on_default', False):\n",
    "                    cfg[arg][i] = None\n",
    "                else:\n",
    "\n",
    "                    # Lower case the keys\n",
    "                    if arg_map[arg].get('lower_case_keys', False):\n",
    "                        item = {key.lower(): val for key, val in item.items()}\n",
    "                    \n",
    "                    if not arg_map[arg].get('skip_arg_map_check', False):\n",
    "                        assert 'arg_map' in arg_map[arg], f'arg_map REQUIRED IN arg_map FOR CONFIG VARIABLE {arg}'\n",
    "                        cfg[arg][i] = validate_cfg(item, arg_map[arg]['arg_map'])\n",
    "            \n",
    "\n",
    "            if as_dict:\n",
    "                cfg[arg] = cfg[arg][0]\n",
    "\n",
    "    extra_keys = list(set(list(cfg.keys())) - set(list(arg_map.keys())))\n",
    "    assert not extra_keys, f'UNKNOWN ARGS {extra_keys}'\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def update_cfg_from_cfg_tbl(cfg: dict, cfg_tbl_name: str) -> dict:\n",
    "    # Stored the keys that were passed so we dont override them\n",
    "    passed_keys = list(cfg.keys())\n",
    "    cfg = validate_cfg(cfg)\n",
    "\n",
    "    # execute_sql_query(\"\"\"\n",
    "    #     CREATE TABLE IF NOT EXISTS users.pamons.validation_config (\n",
    "    #         db_tbl STRING NOT NULL,\n",
    "    #         sf_tbl STRING NOT NULL,\n",
    "    #         primary_keys ARRAY<STRING>,\n",
    "    #         filter_clause STRING,\n",
    "    #         exclude_cols ARRAY<STRING>,\n",
    "    #         db_exclude_cols ARRAY<STRING>,\n",
    "    #         sf_exclude_cols ARRAY<STRING>,\n",
    "    #         column_mapping ARRAY<STRUCT<db: STRING, sf: STRING>>,\n",
    "    #         warehouse_id STRING,\n",
    "    #         cfg_json STRING,\n",
    "    #         user_id STRING\n",
    "    #     ) USING delta\n",
    "    # \"\"\")\n",
    "\n",
    "    cfg_df = spark.sql(f\"SELECT * FROM {cfg_tbl_name} WHERE LOWER(DB_TBL) = LOWER('{cfg['db_tbl'].strip()}') AND LOWER(USER_ID) = LOWER(CURRENT_USER())\")\n",
    "    if cfg_df.count():\n",
    "        print(f\"Found entry for tbl {cfg['db_tbl']} in cfg table {cfg_tbl_name}\")\n",
    "\n",
    "        df_collected = cfg_df.collect()[0]\n",
    "        for arg_name in cfg_df.columns:\n",
    "            if arg_name in ('cfg_json', 'user_id'):\n",
    "                continue\n",
    "\n",
    "            arg_val = df_collected[arg_name]\n",
    "            # Arg should almost always be in cfg because of validate_cfg adding the defaults\n",
    "            # Skip any args where the values match or the arg is not using the default value aka passed via user\n",
    "            if arg_name not in cfg or (arg_val != cfg[arg_name] and arg_name not in passed_keys):\n",
    "                # Handle array of structs\n",
    "                if isinstance(arg_val, list) and len(arg_val) > 0 and isinstance(arg_val[0], Row):\n",
    "                    cfg[arg_name] = [row.asDict() for row in arg_val]\n",
    "                else:\n",
    "                    cfg[arg_name] = arg_val\n",
    "\n",
    "                print(f'Got {arg_name} from cfg tbl with value: {cfg[arg_name]}')\n",
    "        \n",
    "        # Any additional_args we want to load from the json string\n",
    "        additional_args = ['column_exprs']\n",
    "\n",
    "        if additional_args:\n",
    "            cfg_json = json.loads(cfg_df.select(col('cfg_json')).collect()[0][0])\n",
    "            for arg_name in additional_args:\n",
    "                # This should only ever happen if we have never seen an arg before\n",
    "                if arg_name not in cfg_json:\n",
    "                    continue\n",
    "\n",
    "                arg_val = cfg_json[arg_name]\n",
    "\n",
    "                if arg_name not in cfg or (arg_val != cfg[arg_name] and arg_name not in passed_keys):\n",
    "                    cfg[arg_name] = arg_val\n",
    "                    print(f'Got {arg_name} from cfg tbl with value: {cfg[arg_name]}')\n",
    "        \n",
    "    else:\n",
    "        print(f\"Could not find a config record for {cfg['db_tbl']} in {cfg_tbl_name}\")\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def update_cfg_tbl(cfg: dict, cfg_tbl_name: str) -> None:\n",
    "    # print(f'Updating {cfg_tbl_name}')\n",
    "    # Add the cgf in case we need to extract new fields later\n",
    "    cfg.update({'cfg_json': json.dumps(cfg)})\n",
    "\n",
    "    spark.createDataFrame(\n",
    "        [cfg]\n",
    "        , 'db_tbl string, sf_tbl string, primary_keys array<string>, filter_clause string, exclude_cols array<string>, db_exclude_cols array<string>, sf_exclude_cols array<string>, column_mapping array<struct<db:string,sf:string>>, warehouse_id string, cfg_json string'\n",
    "    ).withColumn('user_id', current_user()).createOrReplaceTempView('validation_config_tmp')\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {cfg_tbl_name} a\n",
    "        USING validation_config_tmp b\n",
    "        ON a.db_tbl = b.db_tbl and a.user_id = b.user_id\n",
    "        WHEN MATCHED THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "#### 1 - Schema check ####\n",
    "def col_dt_check(cfg: dict, db_tbl: Table, sf_tbl: Table) -> dict:\n",
    "    print('Starting check #1, compare the column datatypes')\n",
    "    result_dict = {}\n",
    "\n",
    "    # Check using regex because the decimal can have various scales\n",
    "    sf_db_dt_mappings = [\n",
    "        {'db': 'bigint', 'sf': 'decimal\\(\\d+,0\\)'}\n",
    "        ,{'db': 'int', 'sf': 'decimal\\(\\d+,0\\)'}\n",
    "        ,{'db': 'double', 'sf': 'decimal\\(\\d+,d+\\)'}\n",
    "        ,{'db': 'timestamp_ntz', 'sf': 'timestamp'}\n",
    "        ,{'db': 'decimal(38,6)', 'sf': '^decimal\\(38,(\\d+)\\)$'}\n",
    "        ,{'db': 'decimal(38,9)', 'sf': '^decimal\\(38,(\\d+)\\)$'}\n",
    "    ]\n",
    "    sf_db_dt_mapping_df = spark.createDataFrame(sf_db_dt_mappings)\n",
    "\n",
    "    # db_full_tbl_col_info = [['db'] + col for col in get_tbl_col_info(db_tbl_name) if col[0] in col_lst]\n",
    "    # sf_full_tbl_col_info = [['sf'] + col for col in get_tbl_col_info(sf_tbl_name) if col[0] in col_lst]\n",
    "    db_full_tbl_col_info = [['db', col['name'], col['data_type']] for col in db_tbl.col_info]\n",
    "    sf_full_tbl_col_info = [['sf', col['name'], col['data_type']] for col in sf_tbl.col_info]\n",
    "    schema_df = spark.createDataFrame(db_full_tbl_col_info + sf_full_tbl_col_info, 'src string, col_name string, data_type string')\n",
    "\n",
    "    schema_res = (\n",
    "        schema_df.filter(\"src = 'db'\").alias('db')\n",
    "        .join(schema_df.filter(\"src = 'sf'\").alias('sf'), on=[col('db.col_name') == col('sf.col_name')], how='inner')\n",
    "        .join(sf_db_dt_mapping_df.alias('mapping'), on=[(col('db.data_type') == col('db'))], how='left')\n",
    "        .select(\n",
    "            col('db.col_name').alias('column_name'), col('db.data_type').alias('db_dt'), col('sf.data_type').alias('sf_dt')\n",
    "            , ((nvl(col('db.data_type') == col('sf.data_type'), lit(False))) | (nvl(regexp(col('sf.data_type'), col('sf')), lit(False)))).alias('dt_match_flg')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Old way of doing it, much slower, leaving this here so the query can be populated in the result, the result should be the same\n",
    "    # schema_check_query = f\"\"\"\n",
    "    #     SELECT db.column_name, db.data_type db_dt, sf.data_type sf_dt, LOWER(db.data_type) = LOWER(sf.data_type) dt_match_flg\n",
    "    #     FROM {db_catalog_name}.information_schema.columns db\n",
    "    #     JOIN {sf_catalog_name}.information_schema.columns sf\n",
    "    #     ON LOWER(db.column_name) = LOWER(sf.column_name)\n",
    "    #         AND LOWER(db.table_schema) = '{db_schema_name}' AND LOWER(db.table_name) = '{db_tbl_name}' AND LOWER(db.column_name) in {str(col_lst).replace('[', '(').replace(']', ')')}\n",
    "    #         AND LOWER(sf.table_schema) = '{sf_schema_name}' AND LOWER(sf.table_name) = '{sf_tbl_name}' AND LOWER(sf.column_name) in {str(col_lst).replace('[', '(').replace(']', ')')}\n",
    "    # \"\"\"\n",
    "    # schema_res = execute_sql_query(schema_check_query, as_df=True)\n",
    "\n",
    "    if schema_res.select('dt_match_flg').distinct().count() != 1:\n",
    "        if cfg['fail_at_first_check']:\n",
    "            raise Exception(f'Schemas do not match')\n",
    "        result_dict['schema_check'] = {'passed': False}\n",
    "    else:\n",
    "        result_dict['schema_check'] = {'passed': True}\n",
    "    result_dict['schema_check'].update({\n",
    "        'db_col_count': len(db_full_tbl_col_info)\n",
    "        , 'sf_col_count': len(sf_full_tbl_col_info)\n",
    "        , 'missmatched_cols': [(row['column_name'], row['db_dt'], row['sf_dt']) for row in schema_res.filter(\"not dt_match_flg\").collect()]\n",
    "    })\n",
    "\n",
    "    # if cfg['include_queries']:\n",
    "    #     result_dict['schema_check']['query'] = schema_check_query\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#### 2 - Check counts ####\n",
    "def count_check(cfg: dict, db_tbl: Table, sf_tbl: Table) -> dict:\n",
    "    print('Starting check #2, compare the counts of the two tables')\n",
    "    result_dict = {}\n",
    "\n",
    "    count_check_query = f\"\"\"\n",
    "        SELECT 'DB_COUNT' src, COUNT(*) row_cnt FROM {db_tbl.full_copy_name}\n",
    "        UNION ALL\n",
    "        SELECT 'SF_COUNT', COUNT(*) FROM {sf_tbl.full_copy_name}\n",
    "    \"\"\"\n",
    "    count_res = execute_sql_query(count_check_query, as_df=True)\n",
    "\n",
    "    count_dict = {row['src']: row['row_cnt'] for row in count_res.collect()}\n",
    "    if count_dict['DB_COUNT'] != count_dict['SF_COUNT']:\n",
    "        if cfg['fail_at_first_check']:\n",
    "            raise Exception(f'Counts do not match. DB: {count_dict[\"DB_COUNT\"]}, SF: {count_dict[\"SF_COUNT\"]}')\n",
    "        result_dict['count_check'] = {'passed': False}\n",
    "    else:\n",
    "        result_dict['count_check'] = {'passed': True}\n",
    "    result_dict['count_check'].update(count_dict)\n",
    "\n",
    "    if cfg['include_queries']:\n",
    "        result_dict['count_check']['query'] = count_check_query\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#### 3 - Hash check ####\n",
    "# if 6 not in cfg['skip_checks'] and cfg['primary_keys'] and cfg['num_of_sample_rows']\n",
    "# def hash_check(cfg: dict, db_tbl: Table, sf_tbl: Table) -> dict:\n",
    "#     print('Starting check #3, take a hash of the entire row and compare the diffrence of the two tables')\n",
    "#     result_dict = {}\n",
    "\n",
    "#     cols_csv = db_tbl.get_col_csv()\n",
    "\n",
    "#     hash_check_query = f\"\"\"\n",
    "#         SELECT COUNT(*)\n",
    "#         FROM (\n",
    "#             SELECT sha2(concat_ws('||', {cols_csv}), 256) HASHED FROM {db_tbl.full_copy_name}\n",
    "#             MINUS\n",
    "#             SELECT sha2(concat_ws('||', {cols_csv}), 256) FROM {sf_tbl.full_copy_name}\n",
    "#         )\n",
    "#     \"\"\"\n",
    "#     hash_res = execute_sql_query(hash_check_query)\n",
    "#     total_row_count = int(hash_res.result.data_array[0][0])\n",
    "\n",
    "#     if total_row_count != 0:\n",
    "#         if cfg['fail_at_first_check']:\n",
    "#             raise Exception(f'Hash check failed, {total_row_count} rows are different')\n",
    "#         result_dict['hash_check'] = {'passed': False}\n",
    "#     else:\n",
    "#         result_dict['hash_check'] = {'passed': True}\n",
    "#     result_dict['hash_check']['row_diff'] = total_row_count\n",
    "    \n",
    "#     if cfg['include_queries']:\n",
    "#         result_dict['hash_check']['query'] = hash_check_query\n",
    "    \n",
    "#     return result_dict\n",
    "\n",
    "def hash_check(cfg: dict, db_tbl: Table, sf_tbl: Table) -> dict:\n",
    "    print('Starting check #3, take a hash of the entire row and compare the diffrence of the two tables')\n",
    "    result_dict = {}\n",
    "\n",
    "    cols_csv = db_tbl.get_col_csv()\n",
    "\n",
    "    hash_minus_query = f\"\"\"\n",
    "        SELECT sha2(concat_ws('||', {cols_csv}), 256) HASHED FROM {db_tbl.full_copy_name}\n",
    "        MINUS\n",
    "        SELECT sha2(concat_ws('||', {cols_csv}), 256) FROM {sf_tbl.full_copy_name}\n",
    "    \"\"\"\n",
    "    \n",
    "    hash_check_query = f\"SELECT COUNT(*) FROM ({hash_minus_query})\"\n",
    "\n",
    "    hash_res = execute_sql_query(hash_check_query)\n",
    "    total_row_count = int(hash_res.result.data_array[0][0])\n",
    "\n",
    "    if total_row_count != 0:\n",
    "        if cfg['fail_at_first_check']:\n",
    "            raise Exception(f'Hash check failed, {total_row_count} rows are different')\n",
    "        result_dict['hash_check'] = {'passed': False}\n",
    "    else:\n",
    "        result_dict['hash_check'] = {'passed': True}\n",
    "    result_dict['hash_check']['row_diff'] = total_row_count\n",
    "    \n",
    "    if 6 not in cfg['skip_checks'] and not cfg['primary_keys'] and cfg['num_of_sample_rows'] and not result_dict['hash_check']['passed']:\n",
    "        print(f\"Finding {cfg['num_of_sample_rows']} rows from DB where the hashes did not find a match in SF, saving to {cfg['missmatch_tbl_name']}\")\n",
    "        \n",
    "        sample_rows_query = f\"\"\"CREATE OR REPLACE TABLE {cfg['missmatch_tbl_name']} AS\n",
    "            SELECT * FROM {db_tbl.full_copy_name}\n",
    "            WHERE sha2(concat_ws('||', {cols_csv}), 256) IN ({hash_minus_query})\n",
    "            LIMIT {cfg['num_of_sample_rows']}\n",
    "        \"\"\"\n",
    "        \n",
    "        execute_sql_query(sample_rows_query)\n",
    "\n",
    "\n",
    "    if cfg['include_queries']:\n",
    "        result_dict['hash_check']['query'] = hash_check_query\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#### 4 - Check for duplicate primary keys ####\n",
    "def duplicate_pk_check(cfg: dict, db_tbl: Table, sf_tbl: Table) -> dict:\n",
    "    print('Starting check #4, check for duplicate primary keys')\n",
    "    result_dict = {}\n",
    "\n",
    "    duplicate_pk_check_query = f\"\"\"\n",
    "        SELECT lkp_src src, COUNT(DISTINCT pk) as duplicate_key_cnt\n",
    "        FROM (\n",
    "            SELECT 'DB_COUNT' as src, concat_ws('||', {','.join(cfg['primary_keys'])}) pk \n",
    "            FROM {db_tbl.full_copy_name} GROUP BY 2 HAVING COUNT(*) > 1\n",
    "            UNION ALL\n",
    "            SELECT 'SF_COUNT' as src, concat_ws('||', {','.join(cfg['primary_keys'])}) pk \n",
    "            FROM {sf_tbl.full_copy_name} GROUP BY 2 HAVING COUNT(*) > 1\n",
    "        )\n",
    "        -- Add this lookup so we always have a value, even if its 0\n",
    "        RIGHT JOIN (SELECT 'DB_COUNT' AS lkp_src UNION ALL SELECT 'SF_COUNT')\n",
    "        ON src = lkp_src\n",
    "        GROUP BY 1\n",
    "    \"\"\"\n",
    "    duplicate_pk_res = execute_sql_query(duplicate_pk_check_query, as_df=True)\n",
    "\n",
    "    duplicate_pk_dict = {row['src']: row['duplicate_key_cnt'] for row in duplicate_pk_res.collect()}\n",
    "    if duplicate_pk_dict['DB_COUNT'] != 0 or duplicate_pk_dict['SF_COUNT'] != 0:\n",
    "        if cfg['fail_at_first_check']:\n",
    "            # raise Exception(f'Duplicate primary keys found, {duplicate_pk_res.manifest.total_row_count} rows are different')\n",
    "            raise Exception(f'Duplicate primary keys found, DB duplicates: {duplicate_pk_dict[\"DB_COUNT\"]}, SF duplicates: {duplicate_pk_dict[\"SF_COUNT\"]}')\n",
    "        result_dict['duplicate_pk_check'] = {'passed': False}\n",
    "    else:\n",
    "        result_dict['duplicate_pk_check'] = {'passed': True}\n",
    "    result_dict['duplicate_pk_check'].update(duplicate_pk_dict)\n",
    "\n",
    "    if cfg['include_queries']:\n",
    "        result_dict['duplicate_pk_check']['query'] = duplicate_pk_check_query\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#### 5 - Check for rows only in one table based on PKs ####\n",
    "def exclusive_pk_check(cfg: dict, db_tbl: Table, sf_tbl: Table) -> dict:\n",
    "    print('Starting check #5, check for PKs that are exclusive to one table')\n",
    "    result_dict = {}\n",
    "\n",
    "    exclusive_pk_check_query = f\"\"\"\n",
    "        SELECT 'SF' as src, count(*) as exclusive_rows\n",
    "        FROM (\n",
    "            SELECT * FROM {sf_tbl.full_copy_name} sf WHERE NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM {db_tbl.full_copy_name} db\n",
    "                WHERE {' AND '.join([f'sf.{key} <=> db.{key}' for key in cfg['primary_keys']])}\n",
    "            )\n",
    "        )\n",
    "        UNION ALL\n",
    "        SELECT 'DB' as src, count(*) as exclusive_rows\n",
    "        FROM (\n",
    "            SELECT * FROM {db_tbl.full_copy_name} db WHERE NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM {sf_tbl.full_copy_name} sf\n",
    "                WHERE {' AND '.join([f'sf.{key} <=> db.{key}' for key in cfg['primary_keys']])}\n",
    "            )\n",
    "        )\n",
    "    \"\"\"\n",
    "    exclusive_rows_res = execute_sql_query(exclusive_pk_check_query, as_df=True)\n",
    "\n",
    "    exclusive_rows_dict = {row['src']: row['exclusive_rows'] for row in exclusive_rows_res.collect()}\n",
    "    if exclusive_rows_dict['SF'] != 0 or exclusive_rows_dict['DB'] != 0:\n",
    "        if cfg['fail_at_first_check']:\n",
    "            raise Exception(f'Rows exclusive to one table based on PKs, SF: {exclusive_rows_dict[\"SF\"]}, DB: {exclusive_rows_dict[\"DB\"]}')\n",
    "        result_dict['exclusive_pk_check'] = {'passed': False}\n",
    "    else:\n",
    "        result_dict['exclusive_pk_check'] = {'passed': True}\n",
    "    result_dict['exclusive_pk_check'].update(exclusive_rows_dict)\n",
    "\n",
    "    if cfg['include_queries']:\n",
    "        result_dict['exclusive_pk_check']['query'] = exclusive_pk_check_query\n",
    "    \n",
    "    return result_dict\n",
    "\n",
    "\n",
    "#### 6\n",
    "def col_row_level_missmatch_check(cfg: dict, db_tbl: Table, sf_tbl: Table) -> dict:\n",
    "    print('Starting check #6, check where the PKs match but the non PK columns do not')\n",
    "    result_dict = {}\n",
    "\n",
    "    col_lst = db_tbl.get_col_lst()\n",
    "\n",
    "    # Base query that checks for matches in all columns/rows\n",
    "    col_pk_row_mismatch_base_query = f\"\"\"\n",
    "        SELECT {','.join(\n",
    "            [\n",
    "                f'db.{col} {col}_pk' for col in cfg['primary_keys']\n",
    "            ] + [\n",
    "                f'db.{col} {col}_db, sf.{col} {col}_sf, NOT db.{col} <=> sf.{col} {col}_missmatch_flg' for col in col_lst if col not in cfg['primary_keys']\n",
    "            ]\n",
    "        )}\n",
    "        FROM {db_tbl.full_copy_name} db\n",
    "        JOIN {sf_tbl.full_copy_name} sf\n",
    "        ON {' AND '.join([f'db.{key} <=> sf.{key}' for key in cfg['primary_keys']])}\n",
    "            AND ({' OR '.join([f'NOT db.{col} <=> sf.{col}' for col in col_lst if col not in cfg['primary_keys']])})\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a view so that we can cache the results and reuse for the sample rows query\n",
    "    col_pk_row_mismatch_base_tmp_view_name = f\"{cfg['user_schema']}.col_pk_row_mismatch_base__recon_tmp\"\n",
    "    execute_sql_query(f\"\"\"CREATE OR REPLACE TABLE {col_pk_row_mismatch_base_tmp_view_name} as {col_pk_row_mismatch_base_query}\"\"\")\n",
    "\n",
    "    # Query to get the number of missmatches in a column\n",
    "    col_pk_row_mismatch_counts_query = f\"\"\"\n",
    "        SELECT col, count(missmatch_flg) missmatch_cnt\n",
    "        FROM {col_pk_row_mismatch_base_tmp_view_name}\n",
    "        UNPIVOT(missmatch_flg for col in ({','.join([f'{col}_missmatch_flg as {col}' for col in col_lst if col not in cfg['primary_keys']])}))\n",
    "        WHERE missmatch_flg\n",
    "        GROUP BY 1\n",
    "    \"\"\"\n",
    "\n",
    "    col_pk_row_mismatch_counts_res = execute_sql_query(col_pk_row_mismatch_counts_query, as_df=True)\n",
    "\n",
    "    col_missmatch_counts_dict = {row['col']: row['missmatch_cnt'] for row in col_pk_row_mismatch_counts_res.collect()}\n",
    "    if col_pk_row_mismatch_counts_res.count() > 0:\n",
    "        if cfg['fail_at_first_check']:\n",
    "            print(json.dumps(col_missmatch_counts_dict, indent=4))\n",
    "            raise Exception(f'The above {col_pk_row_mismatch_counts_res.count()} columns have missmatches in at least 1 row')\n",
    "        result_dict['col_pk_row_mismatch'] = {'passed': False}\n",
    "    else:\n",
    "        result_dict['col_pk_row_mismatch'] = {'passed': True}\n",
    "    result_dict['col_pk_row_mismatch']['cols_with_missmatched_rows'] = col_missmatch_counts_dict\n",
    "\n",
    "    if cfg['include_queries']:\n",
    "        result_dict['col_pk_row_mismatch']['query'] = col_pk_row_mismatch_counts_query\n",
    "\n",
    "    # Save sample rows\n",
    "    if cfg['num_of_sample_rows'] and col_missmatch_counts_dict:\n",
    "        print(f\"Finding {cfg['num_of_sample_rows']} sample rows for each column with missmatches, saving to {cfg['missmatch_tbl_name']}\")\n",
    "\n",
    "        # Query to generate the delta table\n",
    "        missmatched_rows_query = f\"\"\"\n",
    "            CREATE OR REPLACE TABLE {cfg['missmatch_tbl_name']} AS\n",
    "        \"\"\"\n",
    "\n",
    "        # Old query need to see what is more efficient\n",
    "        missmatched_rows_query += ' UNION ALL '.join([f\"\"\"(\n",
    "                            SELECT DISTINCT {','.join(\n",
    "                                        [\n",
    "                                            f'{key}_pk' for key in cfg['primary_keys']\n",
    "                                        ] + [\n",
    "                                            f\"'{col}' as col_name\", f\"{col}_db::STRING db_val\", f\"{col}_sf::STRING sf_val\"\n",
    "                                        ]\n",
    "                                    )}\n",
    "                            FROM {col_pk_row_mismatch_base_tmp_view_name}\n",
    "                            WHERE {col}_missmatch_flg\n",
    "                            LIMIT {cfg['num_of_sample_rows']}\n",
    "                        )\"\"\" for col in col_missmatch_counts_dict])\n",
    "\n",
    "        execute_sql_query(missmatched_rows_query)\n",
    "    \n",
    "    execute_sql_query(f\"DROP TABLE {col_pk_row_mismatch_base_tmp_view_name}\")\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def set_warehouse_id(tmp_wh_id):\n",
    "    global warehouse_id\n",
    "    warehouse_id = tmp_wh_id\n",
    "\n",
    "def set_spark():\n",
    "    global spark\n",
    "    try:\n",
    "        spark = SparkSession.builder.getOrCreate()\n",
    "    except:\n",
    "        print('Failed to get spark session, creating one using databricks-connect')\n",
    "        \n",
    "        # Wait for the cluster to start\n",
    "        print('Waiting for cluster to start...')\n",
    "        if w.clusters.get(cluster_id).state.value == 'TERMINATED':\n",
    "            w.clusters.start(cluster_id)\n",
    "        w.clusters.wait_get_cluster_running(cluster_id)     \n",
    "        print('Getting spark session...')   \n",
    "        spark = DatabricksSession.builder.clusterId(cluster_id).getOrCreate()\n",
    "\n",
    "\n",
    "def set_globals(warehouse_id=None):\n",
    "    if ('warehouse_id' not in globals() or globals()['warehouse_id'] is None or globals()['warehouse_id'] != warehouse_id) and warehouse_id is not None:\n",
    "        set_warehouse_id(warehouse_id)\n",
    "    if 'spark' not in globals() or globals()['spark'] is None:\n",
    "        set_spark()\n",
    "\n",
    "\n",
    "def reconcile(cfg: dict) -> dict:\n",
    "    # Need to do this twice because we need spark\n",
    "    set_globals()\n",
    "    print('Starting reconcile...')\n",
    "\n",
    "    cfg_copy = validate_cfg({key: val for key, val in cfg.items() if key in ('use_cfg_tbl', 'cfg_tbl_name', 'db_tbl')}, exclude_checks=['sf_tbl'])\n",
    "    if cfg_copy['use_cfg_tbl'] and cfg_copy['cfg_tbl_name']:\n",
    "        cfg = update_cfg_from_cfg_tbl(cfg, cfg_copy['cfg_tbl_name'])\n",
    "\n",
    "    cfg = validate_cfg(cfg)\n",
    "    set_globals(warehouse_id=cfg['warehouse_id'])\n",
    "\n",
    "    current_user = execute_sql_query(\"select replace(current_user(), '@dropbox.com', '')\", as_df=True).collect()[0][0]\n",
    "    user_schema = f'users.{current_user}'\n",
    "\n",
    "    # Test to make sure the schema exists\n",
    "    execute_sql_query(f'show tables in {user_schema}')\n",
    "\n",
    "    cfg['user_schema'] = user_schema\n",
    "\n",
    "    db_tbl = Table(\n",
    "        cfg['db_tbl'].lower().strip()\n",
    "        , full_copy_name=f'{user_schema}.%TBL_NAME%__reconcile_tmp_databricks'\n",
    "        , exclude_cols=cfg['exclude_cols'] + cfg['db_exclude_cols']\n",
    "        , col_mapping={mapping['db']: mapping['sf'] for mapping in cfg['column_mapping']}\n",
    "        , column_exprs=cfg['column_exprs']\n",
    "        , data_type_exprs=cfg['data_type_exprs']\n",
    "    )\n",
    "\n",
    "    sf_tbl = Table(\n",
    "        cfg['sf_tbl'].lower().strip()\n",
    "        , full_copy_name=f'{user_schema}.%TBL_NAME%__reconcile_tmp_snowflake'\n",
    "        , exclude_cols=cfg['exclude_cols'] + cfg['sf_exclude_cols']\n",
    "        , col_mapping={mapping['sf']: mapping['db'] for mapping in cfg['column_mapping']}\n",
    "        , column_exprs=cfg['column_exprs']\n",
    "        , data_type_exprs=cfg['data_type_exprs']\n",
    "    )\n",
    "\n",
    "    cfg['missmatch_tbl_name'] = f'{user_schema}.{db_tbl.tbl_name}__missmatches'\n",
    "\n",
    "    db_col_lst = db_tbl.get_col_lst()\n",
    "    sf_col_lst_alias = sf_tbl.get_col_lst(aliased=True)\n",
    "\n",
    "    cfg['primary_keys'] = [key.lower() for key in cfg['primary_keys']]\n",
    "\n",
    "    # Check to make sure primary keys are not excluded and exist in the tbl\n",
    "    db_missing_pk_keys = list(set(cfg['primary_keys']) - set(db_col_lst))\n",
    "    sf_missing_pk_keys = list(set(cfg['primary_keys']) - set(sf_col_lst_alias))\n",
    "    if len(db_missing_pk_keys) != 0 or len(sf_missing_pk_keys) != 0:\n",
    "        raise Exception(f'Primary keys excluded or does not exists in db or sf table:\\nDB missing keys: {db_missing_pk_keys}\\nSF missing keys: {sf_missing_pk_keys}')\n",
    "\n",
    "\n",
    "    # print((set(db_cols) - set(sf_cols)) | (set(sf_cols) - set(db_cols)))\n",
    "    if set(db_col_lst) != set(sf_col_lst_alias):\n",
    "        raise Exception(f'THE COLUMNS DO NOT MATCH, DB EXCLUSIVE COLS: {set(db_col_lst) - set(sf_col_lst_alias)}, SF EXCLUSIVE COLS: {set(sf_col_lst_alias) - set(db_col_lst)}')\n",
    "\n",
    "    # Create a view in delta in case there is a filter clause\n",
    "    filter_clause = ''\n",
    "    if cfg['filter_clause']:\n",
    "        filter_clause = f\" WHERE {cfg['filter_clause']}\"\n",
    "\n",
    "\n",
    "    # Copy the sf table to delta so we can query it faster\n",
    "    if cfg['create_sf_copy']:\n",
    "        print(f\"Creating {cfg['sf_copy_obj_type'].lower()}: {sf_tbl.full_copy_name} for SF table: {sf_tbl.full_tbl_name}\")\n",
    "        execute_sql_query(sf_tbl.get_copy_ddl(cfg['sf_copy_obj_type'], filter_clause=filter_clause, aliased=True))\n",
    "\n",
    "    # Create a view for the DB table so that we can have a filter\n",
    "    if cfg['create_db_copy']:\n",
    "        print(f\"Creating {cfg['db_copy_obj_type'].lower()}: {db_tbl.full_copy_name} for DB table: {db_tbl.full_tbl_name}\")\n",
    "        execute_sql_query(db_tbl.get_copy_ddl(cfg['db_copy_obj_type'], filter_clause=filter_clause))\n",
    "\n",
    "\n",
    "    # Start the validation\n",
    "    thread_lst = ThreadList()\n",
    "    result_dict = {}\n",
    "    if 1 not in cfg['skip_checks']:\n",
    "        if cfg['use_threads']:\n",
    "            thread_lst.append(ThreadWithReturnValue(target=col_dt_check, args=(cfg, db_tbl, sf_tbl)))\n",
    "        else:\n",
    "            result_dict.update(col_dt_check(cfg, db_tbl, sf_tbl))\n",
    "\n",
    "    if 2 not in cfg['skip_checks']:\n",
    "        if cfg['use_threads']:\n",
    "            thread_lst.append(ThreadWithReturnValue(target=count_check, args=(cfg, db_tbl, sf_tbl)))\n",
    "        else:\n",
    "            result_dict.update(count_check(cfg, db_tbl, sf_tbl))\n",
    "\n",
    "    if 3 not in cfg['skip_checks']:\n",
    "        if cfg['use_threads']:\n",
    "            thread_lst.append(ThreadWithReturnValue(target=hash_check, args=(cfg, db_tbl, sf_tbl)))\n",
    "        else:\n",
    "            result_dict.update(hash_check(cfg, db_tbl, sf_tbl))\n",
    "\n",
    "\n",
    "    # Primary key checks\n",
    "    if cfg['primary_keys']:\n",
    "        result_dict['primary_keys'] = cfg['primary_keys']\n",
    "\n",
    "        if 4 not in cfg['skip_checks']:   \n",
    "            if cfg['use_threads']:\n",
    "                thread_lst.append(ThreadWithReturnValue(target=duplicate_pk_check, args=(cfg, db_tbl, sf_tbl)))\n",
    "            else:\n",
    "                result_dict.update(duplicate_pk_check(cfg, db_tbl, sf_tbl))\n",
    "\n",
    "        if 5 not in cfg['skip_checks']:\n",
    "            if cfg['use_threads']:\n",
    "                thread_lst.append(ThreadWithReturnValue(target=exclusive_pk_check, args=(cfg, db_tbl, sf_tbl)))\n",
    "            else:\n",
    "                result_dict.update(exclusive_pk_check(cfg, db_tbl, sf_tbl))\n",
    "\n",
    "        if 6 not in cfg['skip_checks']:\n",
    "            if cfg['use_threads']:\n",
    "                thread_lst.append(ThreadWithReturnValue(target=col_row_level_missmatch_check, args=(cfg, db_tbl, sf_tbl)))\n",
    "            else:\n",
    "                result_dict.update(col_row_level_missmatch_check(cfg, db_tbl, sf_tbl))\n",
    "\n",
    "    if thread_lst:\n",
    "        thread_lst.start_all_threads()\n",
    "        result_dict.update(thread_lst.join_all_threads())\n",
    "\n",
    "    if cfg['drop_sf_copy']:\n",
    "        print(f\"Dropping SF tmp {cfg['sf_copy_obj_type'].lower()}: {sf_tbl.full_copy_name}\")\n",
    "        execute_sql_query(f\"DROP {cfg['sf_copy_obj_type']} {sf_tbl.full_copy_name}\")\n",
    "\n",
    "    if cfg['drop_db_copy']:\n",
    "        print(f\"Dropping DB tmp {cfg['db_copy_obj_type'].lower()}: {db_tbl.full_copy_name}\")\n",
    "        execute_sql_query(f\"DROP {cfg['db_copy_obj_type']} {db_tbl.full_copy_name}\")\n",
    "\n",
    "    if cfg['cfg_tbl_name'] is not None:\n",
    "        update_cfg_tbl(cfg, cfg['cfg_tbl_name'])\n",
    "\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def get_tbl_col_counts(full_tbl_name: str, warehouse_id: str, results_tbl: str='users.pamons.tbl_column_counts_tbl') -> None:\n",
    "    set_globals(warehouse_id)\n",
    "    \n",
    "    res = []\n",
    "    db_tbl = Table(full_tbl_name.lower().strip())\n",
    "\n",
    "    tbl_row_count = execute_sql_query(f'SELECT COUNT(*) FROM {db_tbl.full_tbl_name}', as_df=True, warehouse_id=warehouse_id).collect()[0][0]\n",
    "\n",
    "    # Need to do this before the exclude cols\n",
    "    all_cols_count = f\"COUNT(DISTINCT CONCAT_WS('||', {db_tbl.get_col_csv()})) as all_cols_distinct_cnt\"\n",
    "\n",
    "    # You can auto exclude bool cols from being a key, they can only have 3 values: true, false, null\n",
    "    db_tbl.set_exclude_cols([col['name'] for col in db_tbl.col_info if col['data_type'] == 'boolean'])\n",
    "\n",
    "    # Search for columns that have unique values\n",
    "    individual_col_key_query = f\"\"\"\n",
    "        SELECT col_name, distinct_val_count, {tbl_row_count} = distinct_val_count AS is_pk\n",
    "        FROM (\n",
    "            SELECT {','.join(\n",
    "                [all_cols_count]\n",
    "                +\n",
    "                [f'COUNT(DISTINCT {col_name}) as {col_name}_distinct_cnt' for col_name in db_tbl.get_col_lst()]\n",
    "            )}\n",
    "            FROM {db_tbl.full_tbl_name}\n",
    "        )\n",
    "        UNPIVOT(DISTINCT_VAL_COUNT FOR COL_NAME IN ({','.join(\n",
    "            ['all_cols_distinct_cnt AS all_cols']\n",
    "            +\n",
    "            [f'{col_name}_distinct_cnt AS {col_name}' for col_name in db_tbl.get_col_lst()]\n",
    "        )}))\n",
    "        ORDER BY distinct_val_count desc\n",
    "    \"\"\"\n",
    "    individual_col_key_df = execute_sql_query(individual_col_key_query, as_df=True, warehouse_id=warehouse_id)\n",
    "\n",
    "    for row in individual_col_key_df.collect():\n",
    "        tmp_res = {'tbl_name': db_tbl.full_tbl_name, 'col_name': row['col_name'].lower()}\n",
    "        tmp_res.update({col: row[col] for col in individual_col_key_df.columns})\n",
    "        res.append(tmp_res)\n",
    "\n",
    "    res_df = spark.createDataFrame(res\n",
    "        , ', '.join(['tbl_name string'] + [' '.join(col) for col in individual_col_key_df.dtypes])\n",
    "    )\n",
    "    \n",
    "    (\n",
    "        res_df.write.mode('overwrite')\n",
    "        .option('replaceWhere', f'tbl_name = \"{db_tbl.full_tbl_name}\"')\n",
    "        .option('overwriteSchema', True)\n",
    "        .saveAsTable(results_tbl)\n",
    "    )\n",
    "\n",
    "    primary_key_count = res_df.filter(col('is_pk')).count()\n",
    "    if primary_key_count == 0:\n",
    "        print(f'Could not find PK for tbl {db_tbl.full_tbl_name}')\n",
    "    else:\n",
    "        print(f'Found {primary_key_count} PKs for tbl {db_tbl.full_tbl_name}')\n",
    "\n",
    "    print(f'Writing results to {results_tbl}')\n",
    "\n",
    "\n",
    "#### find_primary_keys helper functions ####\n",
    "def divide_list_into_chunks(l, n): \n",
    "    for i in range(0, len(l), n):  \n",
    "        yield l[i:i + n] \n",
    "\n",
    "\n",
    "def test_col_chunk(tbl_name: str, col_chunk: list, exclude_cols: list, warehouse_id: str):\n",
    "    except_query = f\"except({', '.join(exclude_cols + col_chunk)})\"\n",
    "    query = f\"\"\"select count(*) from (select * {except_query} from {tbl_name} group by all having count(*) > 1)\"\"\"\n",
    "    return int(execute_sql_query(query, warehouse_id=warehouse_id).result.data_array[0][0])\n",
    "\n",
    "\n",
    "def find_primary_keys(tbl_name, warehouse_id: str, update_tbl_counts: bool=True, tbl_column_counts_tbl_name: str='users.pamons.tbl_column_counts_tbl'):\n",
    "    set_globals(warehouse_id)\n",
    "\n",
    "    if update_tbl_counts:\n",
    "        get_tbl_col_counts(tbl_name, warehouse_id=warehouse_id, results_tbl=tbl_column_counts_tbl_name)\n",
    "    \n",
    "    tbl_res_exist = spark.table(tbl_column_counts_tbl_name) \\\n",
    "        .filter((col('tbl_name') == tbl_name))\n",
    "    \n",
    "    if not update_tbl_counts and tbl_res_exist.count() == 0:\n",
    "        raise Exception(f'update_tbl_counts IS FALSE BUT THERE ARE NO RESULTS IN {tbl_column_counts_tbl_name} FOR TBL {tbl_name}')\n",
    "    \n",
    "    # If all the cols combined do not create a PK then it is impossible to create one\n",
    "    all_cols_pk = tbl_res_exist.filter((col('is_pk')) & (lower(col('col_name')) == 'all_cols'))\n",
    "    if all_cols_pk.count() == 0:\n",
    "        print('No PKs possible, all columns combined do no create a PK')\n",
    "        return []\n",
    "\n",
    "    res = [row['col_name'] for row in tbl_res_exist.filter((col('is_pk')) & (lower(col('col_name')) != 'all_cols')).collect()]\n",
    "    if not res:\n",
    "        non_pk_cols_df = tbl_res_exist.filter(~col('is_pk')).orderBy(asc('distinct_val_count'))\n",
    "\n",
    "        col_lst = [row['col_name'] for row in non_pk_cols_df.collect()]\n",
    "        exclude_cols = []\n",
    "        exclude_cols_old = []\n",
    "\n",
    "        chunk_size = len(col_lst)//6\n",
    "        chunk_size = 1 if chunk_size < 1 else chunk_size \n",
    "        print(f'Chunk size: {chunk_size}')\n",
    "        while exclude_cols == exclude_cols_old and len(list(set(col_lst) - set(exclude_cols))):\n",
    "            exclude_cols_old = exclude_cols.copy()\n",
    "\n",
    "            for col_chunk in list(divide_list_into_chunks(col_lst, chunk_size)):\n",
    "                res_count = test_col_chunk(tbl_name, col_chunk, exclude_cols, warehouse_id)\n",
    "                if res_count == 0:\n",
    "                    print('Excluding cols:', ', '.join(col_chunk), 'no duplicates')\n",
    "                    exclude_cols.extend(col_chunk)\n",
    "                else:\n",
    "                    for col_name in col_chunk:\n",
    "                        res_count = test_col_chunk(tbl_name, [col_name], exclude_cols, warehouse_id)\n",
    "                        if res_count == 0:\n",
    "                            print(f'Excluding col: {col_name} no duplicates')\n",
    "                            exclude_cols.append(col_name)\n",
    "\n",
    "        res = [[col for col in col_lst if col not in exclude_cols]]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62b33025-35c3-481d-8e5a-ab09a4b08c44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tables = [\n",
    "\n",
    "{\n",
    "\n",
    "   'db_tbl': 'bi_dev.pamons.dim_core_team_upsell_events'\n",
    "       , 'sf_tbl': 'connection__bi_snowflake.dimensions.dim_core_team_upsell_events'\n",
    "    , 'num_of_sample_rows': 5\n",
    "    , 'warehouse_id': '65a50cfd6840d9db'\n",
    "}\n",
    "\n",
    "# {\n",
    "#     'db_tbl': 'bi_dev.pamons.int_core_video_file_action_events'\n",
    "#     , 'sf_tbl': 'connection__bi_snowflake.intermediate.int_core_video_file_action_events'\n",
    "#     , 'num_of_sample_rows': 5\n",
    "#     , 'warehouse_id': '65a50cfd6840d9db'\n",
    "#     , 'filter_clause': ' event_dt <= \"2024-08-11\"'\n",
    "# }\n",
    "\n",
    "# {\n",
    "#     'db_tbl': 'bi_dev.pamons.int_core_ss_team_churn_volume'\n",
    "#     , 'sf_tbl': 'connection__bi_snowflake.intermediate.int_core_ss_team_churn_volume'\n",
    "#     , 'num_of_sample_rows': 5\n",
    "#     , 'warehouse_id': '65a50cfd6840d9db'\n",
    "#     ,'column_exprs': {'total_arr_ly': 'round({__COLUMN_NAME__}, 2)', 'total_arr': 'round({__COLUMN_NAME__}, 2)'}\n",
    "\n",
    "# },\n",
    "\n",
    "# ,{\n",
    "#     'db_tbl': 'bi_dev.pamons.stg_core_team_attribute'\n",
    "#     , 'sf_tbl': 'connection__bi_snowflake.staging.stg_core_team_attribute'\n",
    "#     , 'num_of_sample_rows': 5\n",
    "#     , 'warehouse_id': '65a50cfd6840d9db'\n",
    "#     , 'exclude_cols': ['process_ts']\n",
    "#     , 'filter_clause': \" report_dt = '2024-07-29'\"\n",
    "# }\n",
    "    \n",
    "# ,{\n",
    "#     'db_tbl': 'bi_dev.pamons.stg_core_teams_total_arr'\n",
    "#     , 'sf_tbl': 'connection__bi_snowflake.staging.stg_core_teams_total_arr'\n",
    "#     , 'num_of_sample_rows': 5\n",
    "#     , 'warehouse_id': '65a50cfd6840d9db'\n",
    "#     , 'db_exclude_cols': ['day']\n",
    "# }\n",
    "    \n",
    "# ,{\n",
    "#     'db_tbl': 'bi_dev.pamons.stg_core_user_attribute'\n",
    "#     , 'sf_tbl': 'connection__bi_snowflake.staging.stg_core_user_attribute'\n",
    "#     , 'num_of_sample_rows': 5\n",
    "#     , 'warehouse_id': '65a50cfd6840d9db'\n",
    "#     , 'filter_clause': \" report_dt = '2024-07-26'\"\n",
    "\n",
    "# }\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44960909-38d3-41dd-9f16-74d1f1db7710",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting reconcile...\nCreating table: users.amuthumani.dim_core_team_upsell_events__reconcile_tmp_snowflake for SF table: connection__bi_snowflake.dimensions.dim_core_team_upsell_events\nCreating view: users.amuthumani.dim_core_team_upsell_events__reconcile_tmp_databricks for DB table: bi_dev.pamons.dim_core_team_upsell_events\nStarting check #1, compare the column datatypes\nStarting check #2, compare the counts of the two tables\nStarting check #3, take a hash of the entire row and compare the diffrence of the two tables\nFinding 5 rows from DB where the hashes did not find a match in SF, saving to users.amuthumani.dim_core_team_upsell_events__missmatches\nDropping SF tmp table: users.amuthumani.dim_core_team_upsell_events__reconcile_tmp_snowflake\nDropping DB tmp view: users.amuthumani.dim_core_team_upsell_events__reconcile_tmp_databricks\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<h3>Count checks</h3><table><tr><td style=\"padding: 5px;\">Test</td><td style=\"padding: 5px;\">Databricks</td><td style=\"padding: 5px;\">Snowflake</td><td style=\"padding: 5px;\">Pass/Fail</td></tr><tr><td style=\"padding: 5px;\">Column Count</td><td style=\"padding: 5px;\">5</td><td style=\"padding: 5px;\">5</td><td style=\"background: magenta; padding: 5px\">Pass</td></tr><tr><td style=\"padding: 5px;\">Record Count</td><td style=\"padding: 5px;\">3009</td><td style=\"padding: 5px;\">3688</td><td style=\"background: cyan; padding: 5px\">Fail</td></tr><tr><td style=\"padding: 5px;\">Hash Missmatch Count</td><td style=\"padding: 5px;\">25</td><td style=\"padding: 5px;\"></td><td style=\"background: cyan; padding: 5px\">Fail</td></tr></table></br><h3>Table schema check</h3><table><tr><td style=\"background: magenta\">Schemas match!</td></tr></table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from validation_tools import reconcile\n",
    "from validation_tools.html_helpers import count_checks, col_schema_check, col_row_missmatch_check\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType, MapType, ArrayType, DateType\n",
    "from datetime import datetime, timedelta\n",
    "import traceback\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "start_time = datetime.now()\n",
    "max_duration = timedelta(hours=10)\n",
    "\n",
    "\n",
    "for table in tables:\n",
    "    current_timestamp = datetime.now()\n",
    "    data ={}\n",
    "    res = {}\n",
    "    try:\n",
    "        res = reconcile(table)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"RECONCILIATION ERRORED OUT\")\n",
    "        print(traceback.format_exc())\n",
    "        data['comments'] = \"ERROR: \" + str(e) + str(traceback.format_exc())\n",
    "    try:\n",
    "        try:\n",
    "            html_attrs = [\n",
    "            f\"<h3>Matched using keys: {', '.join(res['primary_keys'])}</h3>\"\n",
    "            , count_checks(res)\n",
    "            , col_schema_check(res)\n",
    "            , col_row_missmatch_check(res)\n",
    "            ]\n",
    "            displayHTML('</br>'.join(html_attrs))\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                html_attrs = [\n",
    "                count_checks(res)\n",
    "                , col_schema_check(res)\n",
    "                ]\n",
    "                displayHTML('</br>'.join(html_attrs))\n",
    "            except Exception as e:\n",
    "                print(\"****Could not print HTML version ****\" + str(e))\n",
    "        data['count_check_passed'] = res.get('count_check', {}).get('passed')\n",
    "        data['schema_check_passed'] = res.get('schema_check', {}).get('passed')\n",
    "        data['col_pk_row_mismatch_passed'] = res.get('col_pk_row_mismatch', {}).get('passed')\n",
    "        data['hash_check_passed'] = res.get('hash_check', {}).get('passed')\n",
    "\n",
    "        if not data['hash_check_passed']:\n",
    "            data['comments'] = f\"Hash check failed, check table users.amuthumani.{table['db_tbl'].split('.')[-1]}__hash_mismatches for samples\"\n",
    "\n",
    "        if 'mismatched_rows_dict' in res.get('col_pk_row_mismatch', {}):   \n",
    "            data['mismatched_rows'] = res['col_pk_row_mismatch']['mismatched_rows_dict']\n",
    "\n",
    "        if not res.get('col_pk_row_mismatch', {}).get('passed') and (table.get('primary_keys', None) is None):\n",
    "            data['comments'] = f\"Column PK/Row mismatch check failed. Check table users.amuthumani.{table['db_tbl'].split('.')[-1]} for mismatched rows.\"\n",
    "\n",
    "        if (res.get('hash_check', {}).get('passed') and \n",
    "            res.get('schema_check', {}).get('passed') and \n",
    "            res.get('count_check', {}).get('passed') and \n",
    "            res.get('col_pk_row_mismatch', {}).get('passed')):\n",
    "            data['passed'] = True\n",
    "        elif (res.get('hash_check', {}).get('passed') and \n",
    "            res.get('schema_check', {}).get('passed') and \n",
    "            res.get('count_check', {}).get('passed') and \n",
    "            (table.get('primary_keys', []) == [])):\n",
    "            data['passed'] = True\n",
    "        else:\n",
    "            data['passed'] = False\n",
    "\n",
    "        # Add schema_check details\n",
    "        schema_check = res.get('schema_check', {})\n",
    "        data['schema_check_db_col_count'] = schema_check.get('db_col_count')\n",
    "        data['schema_check_sf_col_count'] = schema_check.get('sf_col_count')\n",
    "        data['schema_check_missmatched_cols'] = str(schema_check.get('missmatched_cols', ''))\n",
    "\n",
    "        count_check = res.get('count_check', {})\n",
    "        data['count_check_db_count'] = count_check.get('DB_COUNT')\n",
    "        data['count_check_sf_count'] = count_check.get('SF_COUNT')\n",
    "\n",
    "        hash_check = res.get('hash_check', {})\n",
    "        data['hash_check_row_diff'] = hash_check.get('row_diff')\n",
    "\n",
    "        data['primary_keys'] = res.get('primary_keys')\n",
    "\n",
    "        # Add duplicate_pk_check details\n",
    "        duplicate_pk_check = res.get('duplicate_pk_check', {})\n",
    "        data['duplicate_pk_check_db_count'] = duplicate_pk_check.get('DB_COUNT')\n",
    "        data['duplicate_pk_check_sf_count'] = duplicate_pk_check.get('SF_COUNT')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERRORED OUT\")\n",
    "        print(traceback.format_exc())\n",
    "        data['comments'] = \"ERROR: \" + str(e) + str(traceback.format_exc())\n",
    "\n",
    "    data['db_tbl'] = table['db_tbl']\n",
    "    data['sf_tbl'] = table['sf_tbl']\n",
    "    data['report_ts'] = current_timestamp\n",
    "\n",
    "    results.append(data)\n",
    "    # # Check if 4 hours have passed\n",
    "    # if datetime.now() - start_time > max_duration:\n",
    "    #     print(\"4 hours have passed. Ending the loop.\")\n",
    "    #     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e9da99e-3461-4c53-811f-d17ffdf9f218",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'count_check_passed': False, 'schema_check_passed': True, 'col_pk_row_mismatch_passed': None, 'hash_check_passed': False, 'comments': 'Hash check failed, check table users.amuthumani.dim_core_team_upsell_events__hash_mismatches for samples', 'passed': False, 'schema_check_db_col_count': 5, 'schema_check_sf_col_count': 5, 'schema_check_missmatched_cols': '[]', 'count_check_db_count': 3009, 'count_check_sf_count': 3688, 'hash_check_row_diff': 25, 'primary_keys': None, 'duplicate_pk_check_db_count': None, 'duplicate_pk_check_sf_count': None, 'db_tbl': 'bi_dev.pamons.dim_core_team_upsell_events', 'sf_tbl': 'connection__bi_snowflake.dimensions.dim_core_team_upsell_events', 'report_ts': datetime.datetime(2024, 8, 15, 19, 41, 4, 362810)}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5103bff7-2530-47c3-892a-dcc930fbc4f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0981b26f-8203-4e23-8b1e-a4efe5e1b5cf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, BooleanType, MapType, ArrayType, DateType, LongType\n",
    "\n",
    "# Starting check #1, compare the column datatypes\n",
    "# Starting check #2, compare the counts of the two tables\n",
    "# Starting check #3, take a hash of the entire row and compare the diffrence of the two tables\n",
    "# Starting check #4, check for duplicate primary keys\n",
    "# Starting check #5, check for PKs that are exclusive to one table\n",
    "# Starting check #6, check where the PKs match but the non PK columns do not\n",
    "schema = StructType([\n",
    "    \n",
    "    StructField(\"db_tbl\", StringType(), True),\n",
    "    StructField(\"sf_tbl\", StringType(), True),\n",
    "    StructField(\"report_ts\", TimestampType(), True),\n",
    "    StructField(\"passed\", BooleanType(), True),\n",
    "    StructField(\"primary_keys\", ArrayType(StringType()), True),\n",
    "    StructField(\"duplicate_pk_check_db_count\", LongType(), True),\n",
    "    StructField(\"duplicate_pk_check_sf_count\", LongType(), True),\n",
    "    StructField(\"count_check_passed\", BooleanType(), True),\n",
    "    StructField(\"count_check_db_count\", LongType(), True),\n",
    "    StructField(\"count_check_sf_count\", LongType(), True),\n",
    "    StructField(\"schema_check_passed\", BooleanType(), True),\n",
    "    StructField(\"schema_check_db_col_count\", LongType(), True),\n",
    "    StructField(\"schema_check_sf_col_count\", LongType(), True),\n",
    "    StructField(\"schema_check_missmatched_cols\", StringType(), True),\n",
    "    \n",
    "    StructField(\"hash_check_passed\", BooleanType(), True),\n",
    "    StructField(\"hash_check_row_diff\", LongType(), True),\n",
    "    StructField(\"col_pk_row_mismatch_passed\", BooleanType(), True),\n",
    "    StructField(\"mismatched_rows\", MapType(\n",
    "        StringType(), ArrayType(StructType([\n",
    "            StructField(\"report_dt_pk\", DateType(), True),\n",
    "            StructField(\"col_name\", StringType(), True),\n",
    "            StructField(\"db_val\", StringType(), True),\n",
    "            StructField(\"sf_val\", StringType(), True)\n",
    "        ]))\n",
    "    ), True),\n",
    "    StructField(\"comments\", StringType(), True)\n",
    "])\n",
    "# Convert to DataFrame\n",
    "df = spark.createDataFrame(results, schema)\n",
    "\n",
    "# Write to a table\n",
    "output_table_name = \"bi_dev.amuthumani.comparison_results_delta\"\n",
    "# df.write.mode(\"overwrite\").saveAsTable(output_table_name)\n",
    "df.write.format(\"delta\").mode(\"append\").saveAsTable(output_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3a062ba-f9d7-407f-ad0b-41746a87f786",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def columns_to_join(tables, i):\n",
    "    # Define the table name\n",
    "    db_tbl = tables[i]['db_tbl']\n",
    "    mismtach_table_name = f\"users.amuthumani.{db_tbl.split('.')[-1]}__missmatches\"\n",
    "\n",
    "    # Load the table schema\n",
    "    mismatch_df = spark.table(mismtach_table_name)\n",
    "\n",
    "    # Get the schema (list of column names and types)\n",
    "    schema = mismatch_df.schema\n",
    "\n",
    "    # Define the suffixes to search for\n",
    "    suffixes = ('_dt', '_id')\n",
    "\n",
    "    # Find columns ending with the specified suffixes\n",
    "    matching_columns = [field.name for field in schema if field.name.endswith(suffixes)]\n",
    "\n",
    "\n",
    "    # Print all column names comma seperated\n",
    "    print(\"All column names:\")\n",
    "    print(\", d.\".join(schema.names))\n",
    "    print()\n",
    "    print(\"join columns\", matching_columns)\n",
    "\n",
    "    return mismtach_table_name, matching_columns, schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d437180-0214-4ffb-9dbf-8236e50dfc2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_sql(mismatch_table_name, cols, all_column_names, table):\n",
    "  # Base tables\n",
    "  sf_table = table['sf_tbl']\n",
    "\n",
    "  db_table = table['db_tbl']\n",
    "\n",
    "  # Generate join conditions using cols list\n",
    "  join_conditions = \" and \".join([f\"m.{col} = s.{col.upper()}\" for col in cols])\n",
    "  db_join_conditions = \" and \".join([f\"m.{col} = d.{col.upper()}\" for col in cols])\n",
    "\n",
    "  # SQL query\n",
    "  sql_text = f\"\"\"\n",
    "  SELECT * FROM (\n",
    "    SELECT 'SF' AS DB, s.*\n",
    "    FROM {mismatch_table_name} m \n",
    "    LEFT OUTER JOIN {sf_table} s ON {join_conditions}\n",
    "    UNION ALL\n",
    "    SELECT 'DB' AS DB, d.* \n",
    "    FROM {mismatch_table_name} m \n",
    "    LEFT OUTER JOIN {db_table} d ON {db_join_conditions}\n",
    "  ) \n",
    "  ORDER BY all\n",
    "  LIMIT 100;\n",
    "  \"\"\"\n",
    "  # Exclude columns 'column1' and 'column2'\n",
    "  excluded_columns = table.get('db_exclude_cols', []) + table.get('sf_exclude_cols', []) + ['hashed']\n",
    "\n",
    "  # Get the list of all columns\n",
    "  all_columns = [col for col in all_column_names if col not in excluded_columns]\n",
    "\n",
    "  sql_text = sql_text.replace(', s.*', f', s.{\", s.\".join(all_columns)}')\n",
    "  sql_text = sql_text.replace(', d.*', f', d.{\", d.\".join(all_columns)}')\n",
    "\n",
    "  print(sql_text)\n",
    "  return sql_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71a3eaca-47d4-44a3-8ffa-6352d00c2e3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All column names:\nteam_id, d.day, d.transaction_id, d.upsell_subtype, d.is_idaho_upsell, d.HASHED\n\njoin columns ['team_id', 'transaction_id']\n\n  SELECT * FROM (\n    SELECT 'SF' AS DB, s.team_id, s.day, s.transaction_id, s.upsell_subtype, s.is_idaho_upsell, s.HASHED\n    FROM users.amuthumani.dim_core_team_upsell_events__missmatches m \n    LEFT OUTER JOIN connection__bi_snowflake.dimensions.dim_core_team_upsell_events s ON m.team_id = s.TEAM_ID and m.transaction_id = s.TRANSACTION_ID\n    UNION ALL\n    SELECT 'DB' AS DB, d.team_id, d.day, d.transaction_id, d.upsell_subtype, d.is_idaho_upsell, d.HASHED \n    FROM users.amuthumani.dim_core_team_upsell_events__missmatches m \n    LEFT OUTER JOIN bi_dev.pamons.dim_core_team_upsell_events d ON m.team_id = d.TEAM_ID and m.transaction_id = d.TRANSACTION_ID\n  ) \n  ORDER BY all\n  LIMIT 100;\n  \n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3922396375445145>, line 4\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m mismtach_table_name, matching_columns, all_column_names \u001B[38;5;241m=\u001B[39m columns_to_join(tables, i)\n",
       "\u001B[1;32m      3\u001B[0m sql_text \u001B[38;5;241m=\u001B[39m generate_sql(mismtach_table_name, matching_columns[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m4\u001B[39m], all_column_names, table)\n",
       "\u001B[0;32m----> 4\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(sql_text)\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:733\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m    730\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n",
       "\u001B[1;32m    732\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n",
       "\u001B[0;32m--> 733\u001B[0m data, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n",
       "\u001B[1;32m    734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n",
       "\u001B[1;32m    735\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1270\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1268\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1269\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1270\u001B[0m data, _, _, _, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1271\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1272\u001B[0m )\n",
       "\u001B[1;32m   1273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[1;32m   1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (data\u001B[38;5;241m.\u001B[39mto_pandas(), properties)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1717\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   1714\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   1716\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 1717\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   1718\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   1719\u001B[0m     ):\n",
       "\u001B[1;32m   1720\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   1721\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1693\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   1691\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   1692\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 1693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2009\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2007\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2008\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2009\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2010\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n",
       "\u001B[1;32m   2011\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2097\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2094\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2095\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2097\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2098\u001B[0m                 info,\n",
       "\u001B[1;32m   2099\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2100\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2101\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2102\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2104\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `s`.`HASHED` cannot be resolved. Did you mean one of the following? [`m`.`HASHED`, `s`.`DAY`, `s`.`TEAM_ID`, `m`.`day`, `m`.`team_id`]. SQLSTATE: 42703; line 3 pos 96;\n",
       "'GlobalLimit 100\n",
       "+- 'LocalLimit 100\n",
       "   +- 'Sort ['all ASC NULLS FIRST], true\n",
       "      +- 'Project [*]\n",
       "         +- 'SubqueryAlias __auto_generated_subquery_name\n",
       "            +- 'Union false, false\n",
       "               :- 'Project [SF AS DB#25065, team_id#25089, day#25090, transaction_id#25091, upsell_subtype#25092, is_idaho_upsell#25093, 's.HASHED]\n",
       "               :  +- Join LeftOuter, ((team_id#25083 = TEAM_ID#25089) AND (transaction_id#25085 = TRANSACTION_ID#25091))\n",
       "               :     :- SubqueryAlias m\n",
       "               :     :  +- SubqueryAlias users.amuthumani.dim_core_team_upsell_events__missmatches\n",
       "               :     :     +- Relation users.amuthumani.dim_core_team_upsell_events__missmatches[team_id#25083,day#25084,transaction_id#25085,upsell_subtype#25086,is_idaho_upsell#25087,HASHED#25088] parquet\n",
       "               :     +- SubqueryAlias s\n",
       "               :        +- SubqueryAlias connection__bi_snowflake.dimensions.dim_core_team_upsell_events\n",
       "               :           +- RelationV2[TEAM_ID#25089, DAY#25090, TRANSACTION_ID#25091, UPSELL_SUBTYPE#25092, IS_IDAHO_UPSELL#25093] connection__bi_snowflake.dimensions.dim_core_team_upsell_events connection__bi_snowflake.dimensions.dim_core_team_upsell_events\n",
       "               +- 'Project [DB AS DB#25066, team_id#25100, day#25101, transaction_id#25102, upsell_subtype#25103, is_idaho_upsell#25104, 'd.HASHED]\n",
       "                  +- Join LeftOuter, ((team_id#25094 = TEAM_ID#25100) AND (transaction_id#25096 = TRANSACTION_ID#25102))\n",
       "                     :- SubqueryAlias m\n",
       "                     :  +- SubqueryAlias users.amuthumani.dim_core_team_upsell_events__missmatches\n",
       "                     :     +- Relation users.amuthumani.dim_core_team_upsell_events__missmatches[team_id#25094,day#25095,transaction_id#25096,upsell_subtype#25097,is_idaho_upsell#25098,HASHED#25099] parquet\n",
       "                     +- SubqueryAlias d\n",
       "                        +- SubqueryAlias bi_dev.pamons.dim_core_team_upsell_events\n",
       "                           +- Relation bi_dev.pamons.dim_core_team_upsell_events[team_id#25100,day#25101,transaction_id#25102,upsell_subtype#25103,is_idaho_upsell#25104] parquet\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:353)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:355)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:340)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:298)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:340)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:340)\n",
       "\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:340)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:298)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n",
       "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
       "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
       "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
       "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
       "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
       "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:403)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:168)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:403)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:400)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:260)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:427)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:601)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:143)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:601)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1154)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:600)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:596)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:596)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:254)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:253)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:235)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1187)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1187)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:959)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2939)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2781)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2722)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:318)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:240)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:176)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:342)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:342)\n",
       "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n",
       "\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:235)\n",
       "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:341)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:176)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:126)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:525)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n",
       "\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n",
       "\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:108)\n",
       "\tat scala.util.Using$.resource(Using.scala:269)\n",
       "\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:107)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:524)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `s`.`HASHED` cannot be resolved. Did you mean one of the following? [`m`.`HASHED`, `s`.`DAY`, `s`.`TEAM_ID`, `m`.`day`, `m`.`team_id`]. SQLSTATE: 42703; line 3 pos 96;\n'GlobalLimit 100\n+- 'LocalLimit 100\n   +- 'Sort ['all ASC NULLS FIRST], true\n      +- 'Project [*]\n         +- 'SubqueryAlias __auto_generated_subquery_name\n            +- 'Union false, false\n               :- 'Project [SF AS DB#25065, team_id#25089, day#25090, transaction_id#25091, upsell_subtype#25092, is_idaho_upsell#25093, 's.HASHED]\n               :  +- Join LeftOuter, ((team_id#25083 = TEAM_ID#25089) AND (transaction_id#25085 = TRANSACTION_ID#25091))\n               :     :- SubqueryAlias m\n               :     :  +- SubqueryAlias users.amuthumani.dim_core_team_upsell_events__missmatches\n               :     :     +- Relation users.amuthumani.dim_core_team_upsell_events__missmatches[team_id#25083,day#25084,transaction_id#25085,upsell_subtype#25086,is_idaho_upsell#25087,HASHED#25088] parquet\n               :     +- SubqueryAlias s\n               :        +- SubqueryAlias connection__bi_snowflake.dimensions.dim_core_team_upsell_events\n               :           +- RelationV2[TEAM_ID#25089, DAY#25090, TRANSACTION_ID#25091, UPSELL_SUBTYPE#25092, IS_IDAHO_UPSELL#25093] connection__bi_snowflake.dimensions.dim_core_team_upsell_events connection__bi_snowflake.dimensions.dim_core_team_upsell_events\n               +- 'Project [DB AS DB#25066, team_id#25100, day#25101, transaction_id#25102, upsell_subtype#25103, is_idaho_upsell#25104, 'd.HASHED]\n                  +- Join LeftOuter, ((team_id#25094 = TEAM_ID#25100) AND (transaction_id#25096 = TRANSACTION_ID#25102))\n                     :- SubqueryAlias m\n                     :  +- SubqueryAlias users.amuthumani.dim_core_team_upsell_events__missmatches\n                     :     +- Relation users.amuthumani.dim_core_team_upsell_events__missmatches[team_id#25094,day#25095,transaction_id#25096,upsell_subtype#25097,is_idaho_upsell#25098,HASHED#25099] parquet\n                     +- SubqueryAlias d\n                        +- SubqueryAlias bi_dev.pamons.dim_core_team_upsell_events\n                           +- Relation bi_dev.pamons.dim_core_team_upsell_events[team_id#25100,day#25101,transaction_id#25102,upsell_subtype#25103,is_idaho_upsell#25104] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:355)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:340)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:340)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:403)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:168)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:403)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:400)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:427)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:601)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:601)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1154)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:600)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:596)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:596)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:253)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:235)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1187)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1187)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:959)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2939)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2781)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2722)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:318)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:240)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:176)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:342)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:342)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:235)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:341)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:176)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:126)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:525)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:108)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:107)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:524)"
       },
       "metadata": {
        "errorSummary": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `s`.`HASHED` cannot be resolved. Did you mean one of the following? [`m`.`HASHED`, `s`.`DAY`, `s`.`TEAM_ID`, `m`.`day`, `m`.`team_id`]. SQLSTATE: 42703"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "UNRESOLVED_COLUMN.WITH_SUGGESTION",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42703",
        "startIndex": 115,
        "stopIndex": 122
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-3922396375445145>, line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m mismtach_table_name, matching_columns, all_column_names \u001B[38;5;241m=\u001B[39m columns_to_join(tables, i)\n\u001B[1;32m      3\u001B[0m sql_text \u001B[38;5;241m=\u001B[39m generate_sql(mismtach_table_name, matching_columns[\u001B[38;5;241m0\u001B[39m:\u001B[38;5;241m4\u001B[39m], all_column_names, table)\n\u001B[0;32m----> 4\u001B[0m spark\u001B[38;5;241m.\u001B[39msql(sql_text)\u001B[38;5;241m.\u001B[39mshow()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/session.py:733\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m    730\u001B[0m         _views\u001B[38;5;241m.\u001B[39mappend(SubqueryAlias(df\u001B[38;5;241m.\u001B[39m_plan, name))\n\u001B[1;32m    732\u001B[0m cmd \u001B[38;5;241m=\u001B[39m SQL(sqlQuery, _args, _named_args, _views)\n\u001B[0;32m--> 733\u001B[0m data, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(cmd\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_client))\n\u001B[1;32m    734\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m properties:\n\u001B[1;32m    735\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(CachedRelation(properties[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msql_command_result\u001B[39m\u001B[38;5;124m\"\u001B[39m]), \u001B[38;5;28mself\u001B[39m)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1270\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1268\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1269\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1270\u001B[0m data, _, _, _, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1271\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1272\u001B[0m )\n\u001B[1;32m   1273\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m data \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1274\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m (data\u001B[38;5;241m.\u001B[39mto_pandas(), properties)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1717\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   1714\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   1716\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 1717\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   1718\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   1719\u001B[0m     ):\n\u001B[1;32m   1720\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   1721\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:1693\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   1691\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   1692\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 1693\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2009\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2007\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2008\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2009\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2010\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[1;32m   2011\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot invoke RPC\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mclosed\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(error):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/connect/client/core.py:2097\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2094\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_closed \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2095\u001B[0m             \u001B[38;5;66;03m# END-EDGE\u001B[39;00m\n\u001B[0;32m-> 2097\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2098\u001B[0m                 info,\n\u001B[1;32m   2099\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2100\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2101\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2102\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2104\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(status\u001B[38;5;241m.\u001B[39mmessage) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `s`.`HASHED` cannot be resolved. Did you mean one of the following? [`m`.`HASHED`, `s`.`DAY`, `s`.`TEAM_ID`, `m`.`day`, `m`.`team_id`]. SQLSTATE: 42703; line 3 pos 96;\n'GlobalLimit 100\n+- 'LocalLimit 100\n   +- 'Sort ['all ASC NULLS FIRST], true\n      +- 'Project [*]\n         +- 'SubqueryAlias __auto_generated_subquery_name\n            +- 'Union false, false\n               :- 'Project [SF AS DB#25065, team_id#25089, day#25090, transaction_id#25091, upsell_subtype#25092, is_idaho_upsell#25093, 's.HASHED]\n               :  +- Join LeftOuter, ((team_id#25083 = TEAM_ID#25089) AND (transaction_id#25085 = TRANSACTION_ID#25091))\n               :     :- SubqueryAlias m\n               :     :  +- SubqueryAlias users.amuthumani.dim_core_team_upsell_events__missmatches\n               :     :     +- Relation users.amuthumani.dim_core_team_upsell_events__missmatches[team_id#25083,day#25084,transaction_id#25085,upsell_subtype#25086,is_idaho_upsell#25087,HASHED#25088] parquet\n               :     +- SubqueryAlias s\n               :        +- SubqueryAlias connection__bi_snowflake.dimensions.dim_core_team_upsell_events\n               :           +- RelationV2[TEAM_ID#25089, DAY#25090, TRANSACTION_ID#25091, UPSELL_SUBTYPE#25092, IS_IDAHO_UPSELL#25093] connection__bi_snowflake.dimensions.dim_core_team_upsell_events connection__bi_snowflake.dimensions.dim_core_team_upsell_events\n               +- 'Project [DB AS DB#25066, team_id#25100, day#25101, transaction_id#25102, upsell_subtype#25103, is_idaho_upsell#25104, 'd.HASHED]\n                  +- Join LeftOuter, ((team_id#25094 = TEAM_ID#25100) AND (transaction_id#25096 = TRANSACTION_ID#25102))\n                     :- SubqueryAlias m\n                     :  +- SubqueryAlias users.amuthumani.dim_core_team_upsell_events__missmatches\n                     :     +- Relation users.amuthumani.dim_core_team_upsell_events__missmatches[team_id#25094,day#25095,transaction_id#25096,upsell_subtype#25097,is_idaho_upsell#25098,HASHED#25099] parquet\n                     +- SubqueryAlias d\n                        +- SubqueryAlias bi_dev.pamons.dim_core_team_upsell_events\n                           +- Relation bi_dev.pamons.dim_core_team_upsell_events[team_id#25100,day#25101,transaction_id#25102,upsell_subtype#25103,is_idaho_upsell#25104] parquet\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:353)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:152)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8(CheckAnalysis.scala:355)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$8$adapted(CheckAnalysis.scala:340)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:340)\n\tat scala.collection.immutable.Stream.foreach(Stream.scala:533)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:340)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:298)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:297)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:297)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:233)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:215)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:203)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:171)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:348)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:403)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:168)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:403)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:443)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:400)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:260)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:427)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:601)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:143)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:601)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1154)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:600)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:596)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:596)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:254)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:253)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:235)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:130)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1187)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1187)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:959)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:947)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.executeSQL(SparkConnectPlanner.scala:2939)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleSqlCommand(SparkConnectPlanner.scala:2781)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:2722)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:318)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:240)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:176)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:342)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1180)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:342)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:84)\n\tat org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:235)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:83)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:341)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.executeInternal(ExecuteThreadRunner.scala:176)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.org$apache$spark$sql$connect$execution$ExecuteThreadRunner$$execute(ExecuteThreadRunner.scala:126)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.$anonfun$run$2(ExecuteThreadRunner.scala:525)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:103)\n\tat com.databricks.unity.HandleImpl.$anonfun$runWithAndClose$1(UCSHandle.scala:108)\n\tat scala.util.Using$.resource(Using.scala:269)\n\tat com.databricks.unity.HandleImpl.runWithAndClose(UCSHandle.scala:107)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner$ExecutionThread.run(ExecuteThreadRunner.scala:524)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, table in enumerate(tables):\n",
    "    mismtach_table_name, matching_columns, all_column_names = columns_to_join(tables, i)\n",
    "    sql_text = generate_sql(mismtach_table_name, matching_columns[0:4], all_column_names, table)\n",
    "    spark.sql(sql_text).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfcd584-31f1-4301-8cd9-f0e13898e2ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>DB</th><th>team_id</th><th>day</th><th>transaction_id</th><th>upsell_subtype</th><th>is_idaho_upsell</th></tr></thead><tbody><tr><td>DB</td><td>4134787.00000</td><td>2023-07-19</td><td>751045027</td><td>PLAN PRICE UPGRADE</td><td>true</td></tr><tr><td>DB</td><td>6752627.00000</td><td>2023-09-23</td><td>776920715</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr><tr><td>DB</td><td>7930371.00000</td><td>2023-10-13</td><td>784091301</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr><tr><td>DB</td><td>8173651.00000</td><td>2023-10-18</td><td>785583250</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr><tr><td>DB</td><td>8250435.00000</td><td>2023-10-18</td><td>785857965</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr><tr><td>SF</td><td>4134787.00000</td><td>2023-07-19</td><td>751044294</td><td>PLAN PRICE UPGRADE</td><td>true</td></tr><tr><td>SF</td><td>6752627.00000</td><td>2023-09-23</td><td>776923951</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr><tr><td>SF</td><td>7930371.00000</td><td>2023-10-13</td><td>784092050</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr><tr><td>SF</td><td>8173651.00000</td><td>2023-10-18</td><td>785583280</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr><tr><td>SF</td><td>8250435.00000</td><td>2023-10-18</td><td>785857940</td><td>PLAN PRICE UPGRADE</td><td>false</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "DB",
         "4134787.00000",
         "2023-07-19",
         "751045027",
         "PLAN PRICE UPGRADE",
         true
        ],
        [
         "DB",
         "6752627.00000",
         "2023-09-23",
         "776920715",
         "PLAN PRICE UPGRADE",
         false
        ],
        [
         "DB",
         "7930371.00000",
         "2023-10-13",
         "784091301",
         "PLAN PRICE UPGRADE",
         false
        ],
        [
         "DB",
         "8173651.00000",
         "2023-10-18",
         "785583250",
         "PLAN PRICE UPGRADE",
         false
        ],
        [
         "DB",
         "8250435.00000",
         "2023-10-18",
         "785857965",
         "PLAN PRICE UPGRADE",
         false
        ],
        [
         "SF",
         "4134787.00000",
         "2023-07-19",
         "751044294",
         "PLAN PRICE UPGRADE",
         true
        ],
        [
         "SF",
         "6752627.00000",
         "2023-09-23",
         "776923951",
         "PLAN PRICE UPGRADE",
         false
        ],
        [
         "SF",
         "7930371.00000",
         "2023-10-13",
         "784092050",
         "PLAN PRICE UPGRADE",
         false
        ],
        [
         "SF",
         "8173651.00000",
         "2023-10-18",
         "785583280",
         "PLAN PRICE UPGRADE",
         false
        ],
        [
         "SF",
         "8250435.00000",
         "2023-10-18",
         "785857940",
         "PLAN PRICE UPGRADE",
         false
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "DB",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "team_id",
            "nullable": true,
            "type": "decimal(38,5)"
           },
           {
            "metadata": {},
            "name": "day",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "transaction_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "upsell_subtype",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "is_idaho_upsell",
            "nullable": true,
            "type": "boolean"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 39
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "DB",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "team_id",
         "type": "\"decimal(38,5)\""
        },
        {
         "metadata": "{}",
         "name": "day",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "transaction_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "upsell_subtype",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "is_idaho_upsell",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM (\n",
    "    SELECT 'SF' AS DB, s.team_id, s.day, s.transaction_id, s.upsell_subtype, s.is_idaho_upsell\n",
    "    FROM users.amuthumani.dim_core_team_upsell_events__missmatches m \n",
    "    LEFT OUTER JOIN connection__bi_snowflake.dimensions.dim_core_team_upsell_events s ON m.team_id = s.TEAM_ID and m.day = s.day\n",
    "    UNION ALL\n",
    "    SELECT 'DB' AS DB, d.team_id, d.day, d.transaction_id, d.upsell_subtype, d.is_idaho_upsell\n",
    "    FROM users.amuthumani.dim_core_team_upsell_events__missmatches m \n",
    "    LEFT OUTER JOIN bi_dev.pamons.dim_core_team_upsell_events d ON m.team_id = d.TEAM_ID and m.day = d.day\n",
    "  ) \n",
    "  ORDER BY all\n",
    "  LIMIT 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7c18934-7a0b-4fcf-bc0d-760e57cfd80c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4224424715843962,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "serverless validation3",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
